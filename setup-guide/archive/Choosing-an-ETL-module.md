[**HOME**](Home) > [**SNOWPLOW SETUP GUIDE**](Snowplow setup guide) > [**ETL**](choosing-an-etl-module) 

[[https://d3i6fms1cm1j0i.cloudfront.net/github-wiki/images/3-enrich.png]] 

## Available ETL modules

| **Tracker**                                    | **Description**                                     | **Status**       |
|:-----------------------------------------------|:----------------------------------------------------|:-----------------|
| [Hive ETL](hive-etl-setup)                     | An ETL module that uses Hive on EMR to read data from the Cloudfront collector using a [custom SerDe](https://github.com/snowplow/snowplow/tree/master/3-enrich/hive-etl/snowplow-log-deserializers), and write it into a clean, delimited format in S3 suitable for querying in Hive or uploading to a database (e.g. Infobright). | Production-ready |
| [Scalding / Cascading ETL](scalding-etl-setup)             | An alternative ETL module to the Hive module described above. It performs exactly the same function as the Hive module, but instead of using Hive on EMR, it uses Scalding, the Scala flavor of Cascading | Pre-alpha      |

## Note

All the ETL modules outlined above transform the data from the custom format generated by the [collectors](choosing-a-collector) into one suitable for loading into your [storage database](choosing-a-storage-module) of choice. To complete the transfer of data from collectors to storage, however, for any type of storage except for S3 / Hive,  you will need to run the [storage loader](choosing-a-storage-module), to load the transformed data into your database of choice. This is documented in the [storage](choosing-a-storage-module) section of the setup guide.

## ETL modules on the Snowplow roadmap

We do not have any other ETL modules on the roadmap currently.

## Making a selection

Currently, only the [Hive ETL](hive-etl-setup) is production ready. That is the module used by all Snowplow users to date.