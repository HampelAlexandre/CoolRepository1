This is a page of hints, tips and explanations to help you work with Snowplow. If something looks like a bug in Snowplow but isn't, it will end up on this page too.

1. [EmrEtlRunner failed. What do I do now?](#etl-failure)
2. [Why are browser features missing in IE?](#ie-features)
3. [Hive problem: I upgraded and now queries are not working](#non-hive-format-upgrade)
4. [I need to recreate my table of Snowplow events, how?](#rebuild-database)
5. [I want to recompute my Snowplow events, how?](#recompute-events)
6. [My database load process died during an S3 file copy, help!](#s3-filecopy)
7. [Shredding is failing with File does not exist: hdfs:/local/snowplow/shredded-events](#shred-fail)

<a name="etl-failure"/>
### EmrEtlRunner failed. What do I do now?

EmrEtlRunner has three different ways of failing:

1. The ETL job on Elastic MapReduce fails to start
2. The ETL job starts on Elastic MapReduce but errors part way through
3. One or more S3 file copy operations fail

For help diagnosing and fixing these problems, please see our dedicated [[Troubleshooting jobs on Elastic MapReduce]] wiki page.

<a name="ie-features"/>
### Why are browser features all recorded as null for Internet Explorer?

With the exception of cookies and Java, our JavaScript tracker cannot detect what browser features (PDF, Flash etc) a given instance of Internet Explorer has. This is because IE, unlike the other major browsers, does not populate the `window.navigator.mimeTypes[]` and `navigator.plugins[]` properties.

There are other ways of detecting some browser features (via ActiveX), but these are not advised as they can trigger UAC warnings on Windows.

<a name="non-hive-format-upgrade"/>
### Hive problem: I upgraded and now queries are not working or returning nonsense results

The most likely reason for this is that you have configured your ETL process to output your Snowplow event files in the **non-Hive format** (used to feed Infobright etc). This is typically configured with the following configuration option to EmrEtlRunner:

```yaml
:etl:
    :storage_format: non-hive
```

Unlike the Hive format output, the non-Hive format output for Snowplow event files is **not backwards compatible** for Hive queries. In other words, with the non-Hive format, running a HiveQL query across Snowplow event files generated by two different versions of the ETL process will probably not work.

The solution is to re-run the ETL process across all of your raw Snowplow logs when you upgrade your ETL process.

<a name="rebuild-database"/>
###  I need to recreate my table of Snowplow events, how?

If you have somehow lost or corrupted your Snowplow event store (in Infobright or Redshift), don't panic! 

Fortunately, **Snowplow does not delete any data at any stage of its processing**, so it's all available for you to restore from your archive buckets. 

Here is a simple workflow to use with StorageLoader to re-populate Infobright or Redshift with all of your events:

1. Create a new events table in your database, let's call it `events2`
2. Create a new S3 bucket, let's call it `events-archive2`
3. Edit your StorageLoader's `config.yml` file:
   * Change `:table:` to point to your `events2` table
   * Change `:in:` to point to your existing archive bucket
   * Change `:archive:` to point to your new `events-archive2` bucket
4. Rerun StorageLoader

This should load **all** of your events into your new `events2` table, archiving all events after loading into `events-archive2`.

<a name="recompute-events"/>
###  I want to recompute my Snowplow events, how?

You may well want to recompute all of your Snowplow events, for example if we release a new enrichment (such as geo-IP lookup) and you want it to be run against all of your historical data.

Fortunately, **Snowplow does not delete any data at any stage of its processing**, so the raw data is still available in your archive bucket for you to regenerate your Snowplow events from. 

Here is a simple workflow to use with EmrEtlRunner to regenerate your Snowplow events from your raw collector logs:

1. Create a new S3 bucket, let's call it `events2`
2. Create a new S3 bucket, let's call it `logs-archive2`
3. Edit your EmrEtlRunner's `config.yml` file:
   * Change `:in:` to point to your existing archive bucket
   * Change `:out:` to point to your new `events2` bucket
   * Change `:archive:` to point to your new `logs-archive2` bucket
4. Rerun EmrEtlRunner

This should load **recompute** all of your events into your new `events2` bucket, archiving all events after loading into `events-archive2`. From there you can reload your recomputed events into Infobright or Redshift using StorageLoader.

<a name="s3-filecopy"/>
###  My database load process died during an S3 file copy, help!

Occasionally Amazon S3 fails repeatedly to perform a file operation, eventually causing StorageLoader to die. When this happens, you may see "500 InternalServerErrors", reported by [Sluice] [sluice], which is the library we use to handle S3 file operations.

If this happens, you will need to rerun your StorageLoader process, using the following guidance:

If the job died during the download-to-local step, then:
  1. Delete any files in your download folder
  2. Rerun StorageLoader

If the job died during the archiving step, rerun StorageLoader with the command-line option of `--skip download,delete,load`

<a name="shred-fail"/>
### Shredding is failing with File does not exist: hdfs:/local/snowplow/shredded-event

You are probably seeing an error like this in your EMR job's `syslog`:

```
2014-07-17 02:31:42,198 INFO com.amazon.elasticmapreduce.s3distcp.S3DistCp (main): Running with args: [Ljava.lang.String;@471719b6
2014-07-17 02:31:45,975 FATAL com.amazon.elasticmapreduce.s3distcp.S3DistCp (main): Failed to get source file system
java.io.FileNotFoundException: File does not exist: hdfs:/local/snowplow/shredded-events
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:517)
```

The Hadoop job step that is failing is the copy (using Amazon's S3DistCp utility) of shredded JSONs from your EMR cluster's HDFS file system back to Amazon S3, ready for loading into Redshift. Due to an unfortunate attribute of S3DistCp, it will fail if no files were output for shredding. Possible reasons for this:

1. You are not generating any custom contexts, nor unstructured events and have not enabled link click tracking. Solution: run EmrEtlRunner with `--skip shred`. Remove this `--skip` as/when you know that you do have JSONs to shred.
2. You are trying to send contexts/unstructured events, but something is going wrong. Solution: do a text search on your collector logs and see if you can spot the unstructured events / context there, as per the tracker protocol. Specifically you're looking for query string parameters with 'ue_pr' or 'ue_px' or 'co' or 'cx'. Then review your tracker implementation to fix
3. The rows are failing validation for some reason. In this case you should be able to find the data in your bad rows bucket, with the reason for the validation failure. Update your JSON Schemas in Iglu, or your JSON instances accordingly

[sluice]: https://github.com/snowplow/sluice

[rvm]: https://rvm.io/
[rvmrc]: https://rvm.io/workflow/rvmrc/
[bundler]: https://gembundler.com

[sluice]: https://github.com/snowplow/sluice