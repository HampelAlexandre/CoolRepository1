[**HOME**](Home) > **UPGRADING STEPS**

On this page, we are posting the steps to upgrade sequentially after a Snowplow release with the latest version at the top. Here *sequentially* means from the previous to the following.

For easier navigation, please, follow the links below.

- [Snowplow 76 Changeable Hawk-Eagle](#r76) (**r76**) 2016-01-26
- [Snowplow 75 Long-Legged Buzzard](#r75) (**r75**) 2016-01-02
- [Snowplow 74 European Honey Buzzard](#r74) (**r74**) 2015-12-22
- [Snowplow 73 Cuban Macaw](#r73) (**r73**) 2015-12-04
- [Snowplow 72 Great Spotted Kiwi](#r72) (**r72**) 2015-10-15
- [Snowplow 71 Stork-Billed Kingfisher](#r71) (**r71**) 2015-10-02
- [Snowplow 70 Bornean Green Magpie](#r70) (**r70**) 2015-08-19
- [Snowplow 69 Blue-Bellied Roller](#r69) (**r69**) 2015-07-24
- [Snowplow 68 Turquoise Jay](#r68) (**r68**) 2015-07-23
- [Snowplow 67 Bohemian Waxwing](#r67) (**r67**) 2015-07-13
- [Snowplow 66 Oriental Skylark](#r66) (**r66**) 2015-06-16
- [Snowplow 65 Scarlet Rosefinch](#r65) (**r65**) 2015-05-08

<a name="r76" />
##Snowplow 76 Changeable Hawk-Eagle

This release introduces an event de-duplication process which runs on Hadoop, and also includes an important bug fix for our SendGrid webhook support.

### Upgrade steps

Upgrading to this release is simple - the only changed components are the jar versions for Hadoop Enrich and Hadoop Shred. 

####Configuration file

In the `config.yml` file for your EmrEtlRunner, update your `hadoop_enrich` and `hadoop_shred` job versions like so:

```yaml
  versions:
    hadoop_enrich: 1.5.1 # WAS 1.5.0
    hadoop_shred: 0.7.0 # WAS 0.6.0
    hadoop_elasticsearch: 0.1.0 # Unchanged
```

For a complete example, see our [sample config.yml template][config-yml]. 

### Read more

* [R76 Blog Post](http://snowplowanalytics.com/blog/2016/01/26/snowplow-r76-changeable-hawk-eagle-released/)
* [R76 Release Notes](https://github.com/snowplow/snowplow/releases/r76-changeable-hawk-eagle)

<a name="r75" />
##Snowplow 75 Long-Legged Buzzard

This release lets you warehouse the event streams generated by Urban Airship and SendGrid, and also updates our [web-recalculate](https://github.com/snowplow/snowplow/tree/master/5-data-modeling/sql-runner/redshift/sql/web-recalculate) data model.

####EmrEtlRunner and StorageLoader

The corresponding version of the EmrEtlRunner and StorageLoader are available from our Bintray [here](http://dl.bintray.com/snowplow/snowplow-generic/snowplow_emr_r75_long_legged_buzzard.zip).

In your EmrEtlRunner’s `config.yml` file, update your `hadoop_enrich` job’s version to 1.5.0, like so:

```yaml
  versions:
    hadoop_enrich: 1.5.0 # WAS 1.4.0
```

For a complete example, see our [sample config.yml template][config-yml]. 

####Redshift

You'll need to deploy the Redshift tables for any webhooks you plan on ingesting into Snowplow. You can find the Redshift table deployment instructions on the corresponding webhook setup wiki pages:

- [SendGrid webhook Redshift setup](https://github.com/snowplow/snowplow/wiki/SendGrid-webhook-setup#22-redshift)
- [UrbanAirship Connect webhook Redshift setup](https://github.com/snowplow/snowplow/wiki/Urban-Airship-Connect-webhook-setup#22-redshift)

For more details on this release, please check out the [R75 Release Notes](https://github.com/snowplow/snowplow/releases/tag/r75-long-legged-buzzard) on GitHub. Also you might find interesting the [blog release article](http://snowplowanalytics.com/blog/2016/01/02/snowplow-r75-long-legged-buzzard-released/).

<a name="r74" />
##Snowplow 74 European Honey Buzzard

This release adds a Weather Enrichment to the Hadoop pipeline - making Snowplow the first event analytics platform with built-in weather analytics!

Data provider: [OpenWeatherMap](http://openweathermap.org/)

####Configuration files

To take advantage of this new enrichment, update the `hadoop_enrich` jar version in the `emr` section of your configuration YAML:

```yaml
  versions:
    hadoop_enrich: 1.4.0 # WAS 1.3.0
    hadoop_shred: 0.6.0 # UNCHANGED
    hadoop_elasticsearch: 0.1.0 # UNCHANGED
```

For a complete example, see our [sample config.yml template][config-yml]. 

Make sure to add a [`weather_enrichment_config.json`](https://github.com/snowplow/snowplow/blob/master/3-enrich/config/enrichments/weather_enrichment_config.json) configured as explained [here](Weather-enrichment) into your `enrichments` folder too. The file should conform to this [JSON Schema](https://github.com/snowplow/iglu-central/blob/master/schemas/com.snowplowanalytics.snowplow.enrichments/weather_enrichment_config/jsonschema/1-0-0).

Put the [JSONPaths file](https://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/jsonpaths/org.openweathermap/weather_1.json) into your JSONPaths bucket on S3 to facilitate copying of the weather-related enhancements in to the corresponding Redshift table.

####Redshift

If you are using Snowplow with Amazon Redshift, you will need to deploy the [org_openweathermap_weather_1](https://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/sql/org.openweathermap/weather_1.sql) table into your database.

For more details on this release, please check out the [R74 Release Notes](https://github.com/snowplow/snowplow/releases/tag/r74-european-honey-buzzard) on GitHub. Also you might find interesting the [blog release article](http://snowplowanalytics.com/blog/2015/12/22/snowplow-r74-european-honey-buzzard-with-weather-enrichment-released/).

<a name="r73" />
##Snowplow 73 Cuban Macaw

This release adds the ability to automatically load bad rows from the Snowplow Elastic MapReduce jobflow into [Elasticsearch](https://www.elastic.co/) for analysis, and formally separates the Snowplow enriched event format from the TSV format used to load Redshift.

####EmrEtlRunner and StorageLoader

The corresponding version of the EmrEtlRunner and StorageLoader are available from our Bintray [here](http://dl.bintray.com/snowplow/snowplow-generic/snowplow_emr_r73_cuban_macaw.zip).

####Configuration file

You will need to update the jar versions in the `emr` section of your configuration YAML:

```yaml
  versions:
    hadoop_enrich: 1.3.0 # Version of the Hadoop Enrichment process
    hadoop_shred: 0.6.0 # Version of the Hadoop Shredding process
    hadoop_elasticsearch: 0.1.0 # Version of the Hadoop to Elasticsearch copying process
```

In order to start loading bad rows from the EMR jobflow into Elasticsearch, you will need to add an Elasticsearch target to the `targets` section of your configuration YAML.

```yaml
  targets:
    - name: "Our Elasticsearch cluster" # Name for the target - used to label the corresponding jobflow step
      type: elasticsearch # Marks the database type as Elasticsearch
      host: "ec2-43-1-854-22.compute-1.amazonaws.com" # Elasticsearch host
      database: snowplow # The Elasticsearch index
      port: 9200 # Port used to connect to Elasticsearch
      table: bad_rows # The Elasticsearch type
      es_nodes_wan_only: false # Set to true if using Amazon Elasticsearch Service
      username: # Not required for Elasticsearch
      password: # Not required for Elasticsearch
      sources: # Leave blank or specify: ["s3://out/enriched/bad/run=xxx", "s3://out/shred/bad/run=yyy"]
      maxerror:  # Not required for Elasticsearch
      comprows: # Not required for Elasticsearch
```

Note that the `database` and `table` fields actually contain the index and type respectively where bad rows will be stored.

The `sources` field is an array of buckets from which to load bad rows. If you leave this field blank, then the bad rows buckets created by the current run of the EmrEtlRunner will be loaded. Alternatively you can explicitly specify an array of bad row buckets to load.

####Running EmrEtlRunner

Note these updates to EmrEtlRunner's command-line arguments:

- You can skip loading data into Elasticsearch by running EmrEtlRunner with the `--skip elasticsearch` option
- To run just the Elasticsearch load without any other EmrEtlRunner steps, explicitly skip all other steps using `--skip staging,s3distcp,enrich,shred,archive_raw`
- Note that running EmrEtlRunner with `--skip enrich,shred` will no longer skip the EMR job, since there is still the Elasticsearch step to run
- If you are using Postgres rather than Redshift, you should no longer pass the `--skip shred` option to EmrEtlRunner. This is because the shred step now removes JSON fields from the enriched event TSV.

####Database

Use the appropriate migration script to update your version of the `atomic.events` table to the relevant schema:

- [The Redshift migration script](https://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/sql/migrate_0.7.0_to_0.8.0.sql)
- [The PostgreSQL migration script](https://github.com/snowplow/snowplow/blob/master/4-storage/postgres-storage/sql/migrate_0.6.0_to_0.7.0.sql)

If you are upgrading to this release from an older version of Snowplow, we also provide [Redshift migration scripts][migrate-redshift-sql] to `atomic.events` version 0.8.0 from 0.4.0, 0.5.0 and 0.6.0 versions.

**Warning**: these migration scripts will alter your `atomic.events` table in-place, deleting the `unstruct_event`, `contexts`, and `derived_contexts` columns. We recommend that you make a full backup before running these scripts.

For more details on this release, please check out the [R73 Release Notes](https://github.com/snowplow/snowplow/releases/tag/r73-cuban-macaw) on GitHub. Also you might find interesting the [blog release article](http://snowplowanalytics.com/blog/2015/12/04/snowplow-r73-cuban-macaw-released/).

<a name="r73"></a>
##Snowplow 72 Great Spotted Kiwi

This release adds the ability to track clicks through the Snowplow Clojure Collector, adds a cookie extractor enrichment and introduces new de-duplication queries leveraging [R71](#r71)'s event fingerprint

####Clojure Collector

This release bumps the Clojure Collector to version 1.1.0.

To upgrade to this release:

1. Download the new warfile by right-clicking on [this link](http://s3-eu-west-1.amazonaws.com/snowplow-hosted-assets/2-collectors/clojure-collector/clojure-collector-1.1.0-standalone.war) and selecting “Save As…”
2. Log in to your Amazon Elastic Beanstalk console
3. Browse to your Clojure Collector’s application
4. Click the “Upload New Version” and upload your warfile

####Configuration files

You need to update the version of the Enrich jar in your configuration file:

```yaml
    hadoop_enrich: 1.2.0 # Version of the Hadoop Enrichment process
```

If you wish to use the new cookie extractor enrichment, write a configuration JSON and add it to your `enrichments` folder. The example JSON can be found [here](https://github.com/snowplow/snowplow/blob/master/3-enrich/config/enrichments/cookie_extractor_config.json).

This default configuration is capturing the Scala Stream Collector's own `sp` cookie - in practice you would probably extract other more valuable cookies available on your domain. Each extracted cookie will end up a single derived context following the JSON Schema [`org.ietf/http_cookie/jsonschema/1-0-0`](https://github.com/snowplow/iglu-central/blob/master/schemas/org.ietf/http_cookie/jsonschema/1-0-0).

**Note**: This enrichment only works with events recorded by the Scala Stream Collector - the CloudFront and Clojure Collectors do not capture HTTP headers.

Put the [JSONPaths file](https://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/jsonpaths/org.ietf/http_cookie_1.json) into your JSONPaths bucket on S3 to facilitate copying of the cookie-extracted enhancements in to the corresponding Redshit table.

The corresponding JSONPaths file for `com_snowplowanalytics_snowplow_uri_redirect_1` table could be found [here](https://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/jsonpaths/com.snowplowanalytics.snowplow/uri_redirect_1.json).

####Redshift

If you are using Snowplow with Amazon Redshift and wish to use the new cookie extractor enrichment, you will need to deploy the [`org_ietf_http_cookie_1`](https://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/sql/org.ietf/http_cookie_1.sql) table into your database.

For the new URI redirect functionality, install the [`com_snowplowanalytics_snowplow_uri_redirect_1`](https://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/sql/com.snowplowanalytics.snowplow/uri_redirect_1.sql) table.

For more details on this release, please check out the [R72 Release Notes](https://github.com/snowplow/snowplow/releases/tag/r72-great-spotted-kiwi) on GitHub. Also you might find interesting the [blog release article](http://snowplowanalytics.com/blog/2015/10/15/snowplow-r72-great-spotted-kiwi-released/).

<a name="r71" />
##Snowplow 71 Stork-Billed Kingfisher

This release significantly overhauls Snowplow's handling of time and introduces event fingerprinting to support de-duplication efforts. It also brings our validation of unstructured events and custom context JSONs "upstream" from our Hadoop Shred process into our Hadoop Enrich process.

####EmrEtlRunner and StorageLoader

The latest version of the EmrEtlRunner and StorageLoadeder are available from our Bintray [here](http://dl.bintray.com/snowplow/snowplow-generic/snowplow_emr_r71_stork_billed_kingfisher.zip).

Unzip this file to a sensible location (e.g. `/opt/snowplow-r71`).

####Configuration files

You should update the versions of the Enrich and Shred jars in your [configuration file][config-yml]:

```yaml
    hadoop_enrich: 1.1.0 # Version of the Hadoop Enrichment process
    hadoop_shred: 0.5.0 # Version of the Hadoop Shredding process
```

You should also update the AMI version field:

```yaml
    ami_version: 3.7.0
```

For each of your database targets, you must add the new `ssl_mode` field:

```
  targets:
    - name: "My Redshift database"
      ...
      ssl_mode: disable # One of disable (default), require, verify-ca or verify-full
```

If you wish to use the new event fingerprint enrichment, write a configuration JSON and add it to your `enrichments` folder. The example JSON can be found [here](https://github.com/snowplow/snowplow/blob/master/3-enrich/config/enrichments/event_fingerprint_enrichment.json).

####Database

Use the appropriate migration script to update your version of the `atomic.events` table to the corresponding schema:

- [The Redshift migration script](https://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/sql/migrate_0.6.0_to_0.7.0.sql)
- [The PostgreSQL migration script](https://github.com/snowplow/snowplow/blob/master/4-storage/postgres-storage/sql/migrate_0.5.0_to_0.6.0.sql)

If you are ingesting Cloudfront access logs with Snowplow, use the [Cloudfront access log migration script](https://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/sql/com.amazon.aws.cloudfront/migrate_wd_access_log_1_r3_to_r4.sql) to update your `com_amazon_aws_cloudfront_wd_access_log_1` table.

For more details on this release, please check out the [R71 Release Notes](https://github.com/snowplow/snowplow/releases/tag/r71-stork-billed-kingfisher) on GitHub. Also you might find interesting the [blog release article](http://snowplowanalytics.com/blog/2015/10/02/snowplow-r71-stork-billed-kingfisher-released/).

<a name="r70" />
##Snowplow 70 Bornean Green Magpie

This release focuses on improving our StorageLoader and EmrEtlRunner components and is the first step towards combining the two into a single CLI application.

####EmrEtlRunner and StorageLoader

Download the EmrEtlRunner and StorageLoader from [Bintray](http://dl.bintray.com/snowplow/snowplow-generic/snowplow_emr_r70_bornean_green_magpie.zip).

Unzip this file to a sensible location (e.g. `/opt/snowplow-r70`).

Check that you have a compatible JRE (1.7+) installed:

```sh
$ ./snowplow-emr-etl-runner --version
snowplow-emr-etl-runner 0.17.0
```

####Configuration files

Your two old configuration files will no longer work. Use the aforementioned [`combine_configurations.rb`](https://github.com/snowplow/snowplow/blob/master/3-enrich/emr-etl-runner/config/combine_configurations.rb?_sp=208ac724eb936aa6.1455839078771) script to turn them into a unified configuration file and a resolver JSON.

For reference:

- [`config/iglu_resolver.json`](https://github.com/snowplow/snowplow/blob/master/3-enrich/config/iglu_resolver.json) - example resolver JSON
- [`emr-etl-runner/config/config.yml.sample`][config-yml] - example unified configuration YAML

Note that field names in the unified configuration file no longer start with a colon - so `region: us-east-1` not `:region: us-east-1`.

####Running EmrEtlRunner and StorageLoader

The EmrEtlRunner now **requires** a `--resolver` argument which should be the path to your new resolver JSON.

Also note that when specifying steps to skip using the `--skip` option, the "archive" step has been renamed to "archive_raw" in the EmrEtlRunner and "archive_enriched" in the StorageLoader. This is in preparation for merging the two applications into one.

For more details on this release, please check out the [R70 Release Notes](https://github.com/snowplow/snowplow/releases/tag/r70-bornean-green-magpie) on GitHub. Also you might find interesting the [blog release article](http://snowplowanalytics.com/blog/2015/08/19/snowplow-r70-bornean-green-magpie-released/).

<a name="r69" />
##Snowplow 69 Blue-Bellied Roller

This release contains new and updated SQL data models.

The SQL data models are a standalone and optional part of the Snowplow pipeline. Users who don’t use the SQL data models are therefore not affected by this release.

To implement the SQL data models, first execute the relevant [setup queries](https://github.com/snowplow/snowplow/tree/master/5-data-modeling/sql-runner/redshift/setup) in Redshift. Then use [SQL Runner}(https://github.com/snowplow/sql-runner) to execute the [queries](https://github.com/snowplow/snowplow/tree/master/5-data-modeling/sql-runner/redshift/sql) on a regular basis. SQL Runner is an [open source app](https://github.com/snowplow/sql-runner) that makes it easy to execute SQL statements programmatically as part of the Snowplow data pipeline.

The web and mobile data models come in two variants: `recalculate` and `incremental`.

The `recalculate` models drop and recalculate the derived tables using all events, and can therefore be replaced without having to upgrade the tables.

The `incremental` models update the derived tables using only the events from the most recent batch. The updated `incremental` model comes with a [migration script](https://github.com/snowplow/snowplow/blob/master/5-data-modeling/sql-runner/redshift/migration/web-incremental-1-to-2/migration.sql).

For more details on this release, please check out the [R69 Release Notes](https://github.com/snowplow/snowplow/releases/tag/r69-blue-bellied-roller) on GitHub. Also you might find interesting the [blog release article](http://snowplowanalytics.com/blog/2015/07/24/snowplow-r69-blue-bellied-roller-released/).

<a name="r68" />
##Snowplow 68 Turquoise Jay

This is a small release which adapts the EmrEtlRunner to use the new [Elastic MapReduce](http://aws.amazon.com/elasticmapreduce/) API.

You need to update EmrEtlRunner to the version **0.16.0** on GitHub:

```sh
$ git clone git://github.com/snowplow/snowplow.git
$ git checkout r68-turquoise-jay
$ cd snowplow/3-enrich/emr-etl-runner
$ bundle install --deployment
$ cd ../../4-storage/storage-loader
$ bundle install --deployment
```

For more details on this release, please check out the [R68 Release Notes](https://github.com/snowplow/snowplow/releases/tag/r68-turquoise-jay) on GitHub. Also you might find interesting the [blog release article](http://snowplowanalytics.com/blog/2015/07/23/snowplow-r68-turquoise-jay-released/).

<a name="r67" />
##Snowplow 67 Bohemian Waxwing

This release brings a host of upgrades to our real-time [Amazon Kinesis](http://aws.amazon.com/kinesis/) pipeline as well as the embedding of Snowplow tracking into this pipeline.

The Kinesis apps for r67 Bohemian Waxwing are now all available in a single zip file [here](http://dl.bintray.com/snowplow/snowplow-generic/snowplow_kinesis_r67_bohemian_waxwing.zip). Upgrading will require various configuration changes to each of the three applications’ HOCON configuration files.

####Scala Stream Collector

- Change `collector.sink.kinesis.stream.name` to `collector.sink.kinesis.stream.good` in the HOCON
- Add `collector.sink.kinesis.stream.bad` to the HOCON

####Scala Kinesis Enrich

If you want to include Snowplow tracking for this application please append the following:

```
enrich {

    ...

    monitoring {
        snowplow {
            collector-uri: ""
            collector-port: 80
            app-id: ""
            method: "GET"
        }
    }
}
```

Note that this is a wholly optional section; if you do not want to send application events to a second Snowplow instance, simply do not add this to your configuration file.

For a complete example, see our [`config.hocon.sample`](https://raw.githubusercontent.com/snowplow/snowplow/master/3-enrich/scala-kinesis-enrich/src/main/resources/config.hocon.sample) file.

####Kinesis Elasticsearch Sink

- Add `max-timeout` into the `elasticsearch` fields
- Merge location fields into the `elasticsearch` section
- If you want to include Snowplow Tracking for this application please append the following:

```
sink {

    ...

    monitoring {
        snowplow {
            collector-uri: ""
            collector-port: 80
            app-id: ""
            method: "GET"
        }
    }
}
```

Again, note that Snowplow tracking is a wholly optional section.

For a complete example, see our [`config.hocon.sample`](https://raw.githubusercontent.com/snowplow/snowplow/master/4-storage/kinesis-elasticsearch-sink/src/main/resources/config.hocon.sample) file.

For more details on this release, please check out the [R67 Release Notes](https://github.com/snowplow/snowplow/releases/tag/r67-bohemian-waxwing) on GitHub. Also you might find interesting the [blog release article](http://snowplowanalytics.com/blog/2015/07/13/snowplow-r67-bohemian-waxwing-released/).

<a name="r66" />
##Snowplow 66 Oriental Skylark

This release upgrades our Hadoop Enrichment process to run on Hadoop 2.4, re-enables our Kinesis-Hadoop lambda architecture and also introduces a new scriptable enrichment powered by JavaScript.

####EmrEtlRunner

You need to update EmrEtlRunner to the version **0.15.0** on GitHub:

```sh
$ git clone git://github.com/snowplow/snowplow.git
$ git checkout r66-oriental-skylark
$ cd snowplow/3-enrich/emr-etl-runner
$ bundle install --deployment
$ cd ../../4-storage/storage-loader
$ bundle install --deployment
```

####Configuration file

You need to update your EmrEtlRunner's `config.yml` file to reflect the new Hadoop 2.4.0 and AMI 3.6.0 support:

```
:emr:
  :ami_version: 3.6.0 # WAS 2.4.2
```

And:

```
  :versions:
    :hadoop_enrich: 1.0.0 # WAS 0.14.1
```

####JavaScript scripting enrichment

You can enable this enrichment by creating a self-describing JSON and adding into your `enrichments` folder. The [configuration JSON](https://github.com/snowplow/snowplow/blob/master/3-enrich/config/enrichments/javascript_script_enrichment.json) should validate against the [`javascript_script_config`](https://github.com/snowplow/iglu-central/blob/master/schemas/com.snowplowanalytics.snowplow/javascript_script_config/jsonschema/1-0-0) schema.

For more details on this release, please check out the [R66 Release Notes](https://github.com/snowplow/snowplow/releases/tag/r66-oriental-skylark) on GitHub. Also you might find interesting the [blog release article](http://snowplowanalytics.com/blog/2015/06/16/snowplow-r66-oriental-skylark-released/).

<a name="r65" />
##Snowplow 65 Scarlet Rosefinch

This release greatly improves the speed, efficiency, and reliability of Snowplow’s real-time [Kinesis](http://aws.amazon.com/kinesis/) pipeline.

####Kinesis applications

The Kinesis apps for r65 Scarlet Rosefinch are all available in a single zip file [here](http://dl.bintray.com/snowplow/snowplow-generic/snowplow_kinesis_r65_scarlet_rosefinch.zip).

####Configuration files

Upgrading will require various configuration changes to each of the four applications.

#####Scala Stream Collector

Add `backoffPolic`y and buffer fields to the configuration HOCON.

#####Scala Kinesis Enrich

- Add `backoffPolicy` and `buffer` fields to the configuration HOCON
- Extract the resolver from the configuration HOCON into its own JSON file, which can be stored locally or in DynamoDB
- Update the command line arguments as detailed [here](http://snowplowanalytics.com/blog/2015/05/08/snowplow-r65-scarlet-rosefinch-released/#dynamodb)

#####Kinesis LZO S3 Sink

- Rename the outermost key in the configuration HOCON from "connector" to "sink"
- Replace the "s3/endpoint" field with an "s3/region" field (such as `us-east-1`)

#####Kinesis Elasticsearch Sink

Rename the outermost key in the configuration HOCON from "connector" to "sink"

For more details on this release, please check out the [R65 Release Notes](https://github.com/snowplow/snowplow/releases/tag/r65-scarlet-rosefinch) on GitHub. Also you might find interesting the [blog release article](http://snowplowanalytics.com/blog/2015/05/08/snowplow-r65-scarlet-rosefinch-released/).


[config-yml]: https://github.com/snowplow/snowplow/blob/master/3-enrich/emr-etl-runner/config/config.yml.sample
[migrate-postgre-sql]: https://github.com/snowplow/snowplow/tree/master/4-storage/postgres-storage/sql
[migrate-redshift-sql]: https://github.com/snowplow/snowplow/tree/master/4-storage/redshift-storage/sql
