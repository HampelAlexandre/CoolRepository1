<a name="top" />

[**HOME**](Home) > [**SNOWPLOW SETUP GUIDE**](Setting-up-Snowplow) > [Step 3: Setting up Enrich](Setting-up-enrich) > Configuring storage targets

Snowplow offers the option to configure certain storage targets.
This is done using configuration JSONs. 
When running EmrEtlRunner or StorageLoader, the `--targets` argument should be populated with the filepath of a directory containing your configuration JSONs.
Each storage target JSON file can have arbitrary name, but must conform it's JSON Schema.

Currently supported storage targets are:
Here's a list of currently supported targets, grouped by purpose:

* Enriched data

  * Redshift
  * PostgreSQL

* Failures

  * ElasticSearch

* Duplicate tracking

  * AWS DynamoDB


#### targets

Additionally, you can configure Elasticsearch targets in this section. 
These targets are ignored by StorageLoader, but the EmrEtlRunner will load bad rows (rows which fail the enrichment process) into these targets so you can more easily inspect their associated error information and determine what went wrong.

You can load multiple storage targets.

##### Redshift 

To take each variable in turn:

1. `name`, enter a descriptive name for this Snowplow storage target
2. `host`, the host (endpoint in Redshift parlance) of the databse to
   load.
3. `database`, the name of the database to load
4. `port`, the port of the database to load. 5439 is the default Redshift
   port; 5432 is the default Postgres port
5. `schema`, the name of the database schema which will store your Snowplow tables
6. `username`, the database user to load your Snowplow events with.
   You can leave this blank to default to the user running the script
7. `password`, the password for the database user. Leave blank if there
   is no password
8. `maxerror`, a Redshift-specific setting governing how many load errors
   should be permitted before failing the overall load. See the
   [Redshift `COPY` documentation] [redshift-copy] for more details
9. `comprows`, a Redshift-specific setting defining number of rows to be used as the sample size for compression analysis
10. `sslMode`, determines how to handle encryption for client connections and server certificate verification.      The the following `sslMode` values are supported:
 - `DISABLE`: SSL is disabled and the connection is not encrypted.
 - `REQUIRE`: SSL is required.
 - `VERIFY_CA`: SSL must be used and the server certificate must be verified.
 - `VERIFY_FULL`: SSL must be used. The server certificate must be verified and the server hostname must match the hostname attribute on the certificate.

Note: The difference between `VERIFY_CA` and `VERIFY_FULL` depends on the policy of the root CA. If a public CA is used, `VERIFY_CA` allows connections to a server that somebody else may have registered with the CA to succeed. In this case, `verify-full` should always be used. If a local CA is used, or even a self-signed certificate, using `VERIFY_CA` often provides enough protection.

##### Postgres

To take each variable in turn:

1. `name`, enter a descriptive name for this Snowplow storage target
2. `host`, the host (endpoint in Redshift parlance) of the databse to
   load.
3. `database`, the name of the database to load
4. `port`, the port of the database to load. 5439 is the default Redshift
   port; 5432 is the default Postgres port
5. `schema`, the name of the database schema which will store your Snowplow tables
6. `username`, the database user to load your Snowplow events with.
   You can leave this blank to default to the user running the script
7. `password`, the password for the database user. Leave blank if there
   is no password
8. `sslSode`, determines how to handle encryption for client connections and server certificate verification. The the following `sslMode` values are supported:
 - `DISABLE`: SSL is disabled and the connection is not encrypted.
 - `REQUIRE`: SSL is required.
 - `VERIFY_CA`: SSL must be used and the server certificate must be verified.
 - `VERIFY_FULL`: SSL must be used. The server certificate must be verified and the server hostname must match the hostname attribute on the certificate.
9. `purpose`

##### Elasticsearch

To take each variable in turn:

1. `name`: a descriptive name for this Snowplow storage target
2. `type`: should be "elasticsearch" to signal that this is an Elasticsearch target
3. `port`: The port to load. Normally 9200, should be 80 for Amazon Elasticsearch Service. 
4. `database`: The Elasticsearch index to load
5. `table`: The Elasticsearch type to load
6: `sources`: If this field is left blank, then after the enrich and shred steps of the current job, all bad rows generated by those two steps will be loaded into Elasticsearch. Otherwise, you can provide an array of buckets (such as  `["s3://out/enriched/bad/run=2015-11-04-02-52-59", "s3://out/shred/bad/run=2015-11-04-02-52-59"]` ). All bad row files in those buckets will be loaded into Elasticsearch.
7. es_nodes_wan_only: if this is set to true, the EMR job will disable node discovery. This option is necessary when using Amazon Elasticsearch Service.

For information on setting up Elasticsearch itself, see [[Setting up Amazon Elasticsearch Service]].

