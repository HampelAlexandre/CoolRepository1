[**HOME**](Home) > [**SNOWPLOW TECHNICAL DOCUMENTATION**](Snowplow-technical-documentation) > Enrichment

[[https://d3i6fms1cm1j0i.cloudfront.net/github-wiki/images/snowplow-architecture-3-enrichment.png]]

The Snowplow Enrichment step takes the raw log files generated by the
Snowplow collectors, tidies the data up and enriches it so that it is:

1. Ready to be analysed using EMR
2. Ready to be uploaded into Amazon Redshift, PostgreSQL or some other
alternative storage mechanism for analysis

The current enrichment process provides 2 options for developers to use:

1. Using [Apache Spark][spark] for batch processing of data. This is [Spark Enrich][spark-enrich].
Snowplow uses Amazon's EMR to run the Enrichment process. The regular running of the process (which
is necessary to ensure that up-to-date Snowplow data is available for analysis) is managed by
[EmrEtlRunner][emr-etl-runner], a Ruby application.

2. Using Scala and [Amazon Kinesis][kinesis] for real-time processing
of data.

In this guide, we cover:

1. [The Enrichment Process itself](The-enrichment-process)
2. [How the EmrEtlRunner instruments the regular running of the Enrichment Process][emr-etl-runner]
3. [Stream Enrich](stream-enrich)


[spark]: http://spark.apache.org/
[cascading]: http://www.cascading.org/
[kinesis]: http://aws.amazon.com/kinesis/
[emr-etl-runner]: EmrEtlRunner
[spark-enrich]: https://github.com/snowplow/snowplow/tree/master/3-enrich/spark-enrich
