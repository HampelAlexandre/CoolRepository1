[**HOME**](Home) > [**SNOWPLOW TECHNICAL DOCUMENTATION**](Snowplow technical documentation) > [**Enrichment**](Enrichment) > Enrichment

[[https://d3i6fms1cm1j0i.cloudfront.net/github-wiki/images/3-enrich.png]] 

The Snowplow Enrichment step takes the raw log files generated by the Snowplow collectors, tidies the data up and enriches it so that it is:

1. Ready in S3 to be analysed using EMR
2. Ready to be uploaded into Amazon Redshift, PostgreSQL or some other alternative storage mechanism for analysis

The Enrichment process is written using [Scalding][scalding], a Scala implementation of [Cascading][cascading], an ETL library that's written on top of Hadoop. Historically, we have referred to the Enrichment Process as the [Hadoop-ETL][hadoop-etl], to distinguish it from the [Hive-based ETL][hive-etl] that preceded it. The [Hive ETL][hive-etl] has since been deprecated.

Snowplow uses Amazon's EMR to run the Enrichment process. The regular running of the process (which is necessary to ensure that up-to-date Snowplow data is available for analysis) is managed by [EmrEtlRunner][emr-etl-runner], a Ruby application.

In this guide, we cover:

1. [How the EmrEtlRunner instruments the regular running of the Enrichment Process][emr-etl-runner]
2. [The Enrichment Process itself][enrichment-process]


[scalding]: https://github.com/twitter/scalding
[cascading]: https://github.com/twitter/scalding

[etl-blog-post]: http://snowplowanalytics.com/blog/2013/01/09/from-etl-to-enrichment/
[roadmap]: https://github.com/snowplow/snowplow/wiki/Product-roadmap
[hadoop-branch]: https://github.com/snowplow/snowplow/tree/feature/scalding-etl
[emr-etl-runner]: EmrEtlRunner
[enrichment-process]: Hadoop-ETL
[hadoop-etl]: https://github.com/snowplow/snowplow/tree/master/3-enrich/hadoop-etl
[hive-etl]: https://github.com/snowplow/snowplow/tree/master/3-enrich/hive-etl
